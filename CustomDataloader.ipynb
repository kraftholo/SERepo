{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import soundfile as sf\n",
    "import os, fnmatch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.fft as fft\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal.windows import hann\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Data Normalization\n",
    "def minMaxNorm(wav, eps=1e-8):\n",
    "    max = np.max(abs(wav))\n",
    "    min = np.min(abs(wav))\n",
    "    wav = (wav - min) / (max - min + eps)\n",
    "    return wav\n",
    "\n",
    "class DataConfig():\n",
    "    def __init__(self, \n",
    "                 frameSize = 512, \n",
    "                 stride_length = 32,\n",
    "                 sample_rate = 16000,\n",
    "                 duration = 3,\n",
    "                 n_fft = 512,\n",
    "                 modelBufferFrames = 10,\n",
    "                 batchSize = 32,\n",
    "                 shuffle = True,\n",
    "                 noisyPath = 'dataset/train/',\n",
    "                 cleanPath = 'dataset/y_train/',\n",
    "                 dtype = torch.float64,\n",
    "                 device = 'cpu',\n",
    "                 learningRate = 0.001,\n",
    "                ):\n",
    "        \n",
    "        self.frameSize = frameSize\n",
    "        self.stride_length = stride_length\n",
    "        self.sample_rate = sample_rate\n",
    "        self.duration = duration\n",
    "        self.n_fft = n_fft\n",
    "        self.modelBufferFrames = modelBufferFrames\n",
    "        self.batchSize = batchSize\n",
    "        self.shuffle = shuffle\n",
    "        self.noisyPath = noisyPath\n",
    "        self.cleanPath = cleanPath\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "        self.learningRate = learningRate\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_data = self.inputs[idx]\n",
    "        target_data = self.targets[idx]\n",
    "        return input_data, target_data\n",
    "\n",
    "class CustomDataloaderCreator():\n",
    "    def __init__(self, \n",
    "                 noisy_files_list, \n",
    "                 clean_files_list,\n",
    "                 test_noisy_files_list,\n",
    "                 test_clean_files_list,\n",
    "                 val_noisy_files_list,\n",
    "                 val_clean_files_list,\n",
    "                 dataconfig\n",
    "                 ):\n",
    "        \n",
    "        self.debugFlag = True\n",
    "\n",
    "        #Training Dataset\n",
    "        self.noisy_files_list = noisy_files_list\n",
    "        self.clean_files_list = clean_files_list\n",
    "        self.trainNoisyDataset = []\n",
    "        self.trainCleanDataset = []\n",
    "        self.trainModelInputBuffers = []\n",
    "        self.trainPhaseInfo = []\n",
    "        self.targets = []\n",
    "\n",
    "        #Test Dataset\n",
    "        self.test_noisy_files_list = test_noisy_files_list\n",
    "        self.test_clean_files_list = test_clean_files_list\n",
    "        self.testNoisyDataset = []\n",
    "        self.testCleanDataset = []\n",
    "        \n",
    "        #Validation Dataset\n",
    "        self.val_noisy_files_list = val_noisy_files_list\n",
    "        self.val_clean_files_list = val_clean_files_list\n",
    "        self.validationNoisyDataset = []\n",
    "        self.validationCleanDataset = []\n",
    "        self.val_modelInputBuffers = []\n",
    "        self.val_phaseInfo = []\n",
    "        self.val_targets = []\n",
    "        \n",
    "        self.dataconfig = dataconfig\n",
    "\n",
    "        if(dataconfig.dtype == torch.float64) :\n",
    "            self.dtype = np.float64\n",
    "            self.complexDtype = np.complex128\n",
    "        \n",
    "        elif(dataconfig.dtype == torch.float32) :\n",
    "            self.dtype = np.float32\n",
    "            self.complexDtype = np.complex64\n",
    "        \n",
    "    #Creates the specified duration audio clips from the noisy and clean files\n",
    "    def createAudioClips(self):\n",
    "        print(\"CustomDataLoader.createAudioClips()\")\n",
    "        speechSampleSize = self.dataconfig.duration * self.dataconfig.sample_rate\n",
    "\n",
    "        listIter = [self.noisy_files_list, self.val_noisy_files_list]\n",
    "        datasetIter = [(self.trainNoisyDataset, self.trainCleanDataset), (self.validationNoisyDataset, self.validationCleanDataset)]\n",
    "        NOISY = 0\n",
    "        CLEAN = 1\n",
    "        TEST = 2    #Not used yet\n",
    "\n",
    "        #Create the training and then validation dataset\n",
    "        for index,currlist in enumerate(listIter):\n",
    "            for idx,filename in enumerate(currlist):\n",
    "                if idx == 500:\n",
    "                    break\n",
    "\n",
    "                noisySpeech,_ = sf.read(os.path.join(self.dataconfig.noisyPath, filename))\n",
    "                cleanSpeech,_ = sf.read(os.path.join(self.dataconfig.cleanPath, filename))\n",
    "\n",
    "                #Normalize\n",
    "                noisySpeech = minMaxNorm(noisySpeech)\n",
    "                cleanSpeech = minMaxNorm(cleanSpeech)\n",
    "\n",
    "                numSubSamples = int(len(noisySpeech)/speechSampleSize)\n",
    "                for i in range(numSubSamples):\n",
    "                    datasetIter[index][NOISY].append(noisySpeech[i*speechSampleSize:(i+1)*speechSampleSize])\n",
    "                    datasetIter[index][CLEAN].append(cleanSpeech[i*speechSampleSize:(i+1)*speechSampleSize])\n",
    "        \n",
    "    #This function creates train and validation inputs and targets\n",
    "    # Input : Frequency Domain 10 frame buffer (size 10*framesize)\n",
    "    # Target : Time Domain 2 ms clean speech (size strideLength)\n",
    "    def createModelBufferInputs(self):\n",
    "        print(\"CustomDataLoader.createModelBufferInputs()\")\n",
    "       \n",
    "        fft_freq_bins = int(self.dataconfig.n_fft/2) + 1\n",
    "\n",
    "        datasetIter = [(self.trainNoisyDataset, self.trainCleanDataset), (self.validationNoisyDataset, self.validationCleanDataset)]\n",
    "        modelBufferFramesIter = [self.trainModelInputBuffers, self.val_modelInputBuffers]\n",
    "        targetsIter = [self.targets, self.val_targets]\n",
    "        \n",
    "        NOISY = 0\n",
    "        CLEAN = 1\n",
    "       \n",
    "        #Create the training and validation inputs and targets\n",
    "        for index,data in enumerate(datasetIter):\n",
    "            currNoisyDataset = data[NOISY]\n",
    "            corrCleanDataset = data[CLEAN]\n",
    "            print(f'xFrames (expectedFrames) per audio clip = {len(currNoisyDataset[0])//self.dataconfig.stride_length}')\n",
    "            for idx, currNoisySample in enumerate(tqdm(currNoisyDataset)):\n",
    "                modelInputBuffer = np.zeros((self.dataconfig.modelBufferFrames,fft_freq_bins)).astype(self.complexDtype)\n",
    "                inbuffer = np.zeros((self.dataconfig.frameSize)).astype(self.dtype)\n",
    "\n",
    "                for i in range(0, len(currNoisySample),self.dataconfig.stride_length):      \n",
    "\n",
    "                    if(i+self.dataconfig.stride_length > len(currNoisySample)-1):\n",
    "                        break\n",
    "\n",
    "                    #inbuffer is moved : [__s1__+++++++++++++__s2__] -> [+++++++++++++__s2__]\n",
    "                    inbuffer[:-self.dataconfig.stride_length] = inbuffer[self.dataconfig.stride_length:] \n",
    "                    #inbuffer is filled with new data: [+++++++++++++__s2__] -> [+++++++++++++----]\n",
    "                    inbuffer[-self.dataconfig.stride_length:] = currNoisySample[i : i + self.dataconfig.stride_length]\n",
    "\n",
    "                    #Start up time\n",
    "                    if i < self.dataconfig.frameSize:\n",
    "                        continue\n",
    "\n",
    "                    buffer_array = np.array(inbuffer)\n",
    "                    windowed_buffer = buffer_array * hann(len(buffer_array), sym=False)\n",
    "\n",
    "                    # Taking the real-valued FFT\n",
    "                    frame = np.fft.rfft(windowed_buffer)    \n",
    "\n",
    "                    # if(self.debugFlag):\n",
    "                    #     print(f'frame.shape = {frame.shape}')\n",
    "            \n",
    "                    # Shift the modelInputBuffer\n",
    "                    modelInputBuffer[:-1, :] = modelInputBuffer[1:, :]\n",
    "\n",
    "                    # Fill the last row of modelInputBuffer with the new spectrogram values\n",
    "                    modelInputBuffer[-1, :] = frame\n",
    "                    modelBufferFramesIter[index].append(np.array(modelInputBuffer))\n",
    "                    targetsIter[index].append(np.array(corrCleanDataset[idx][i:i+self.dataconfig.stride_length]))\n",
    "            \n",
    "            # #Shuffle up the dataset if required\n",
    "            # if self.dataconfig.shuffle:\n",
    "            #     self.indices = np.random.permutation(len(modelBufferFramesIter[index]))\n",
    "            # else:\n",
    "            #     self.indices = np.arange(len(modelBufferFramesIter[index]))\n",
    "            # print(\"CustomDataLoader.createModelBufferInputs(): modelinputbuffers size = \", len(self.trainModelInputBuffers))\n",
    "            # print(f'gotten frames per modelinputbuffer = {len(self.trainModelInputBuffers)//len(self.trainNoisyDataset)}')\n",
    "            # print(\"CustomDataLoader.createModelBufferInputs(): targets size = \", len(self.targets))\n",
    "\n",
    "    #This function creates train and validation inputs and targets\n",
    "    # Input : Frequency Domain 10 frame buffer (size 10*framesize)\n",
    "    # Target : Frequency Domain 1 frame buffer (size framesize)\n",
    "    def createModelBufferInputs2(self):\n",
    "        print(\"CustomDataLoader.createModelBufferInputs2()\")\n",
    "       \n",
    "        fft_freq_bins = int(self.dataconfig.n_fft/2) + 1\n",
    "\n",
    "        datasetIter = [(self.trainNoisyDataset, self.trainCleanDataset), (self.validationNoisyDataset, self.validationCleanDataset)]\n",
    "        modelBufferFramesIter = [self.trainModelInputBuffers, self.val_modelInputBuffers]\n",
    "        targetsIter = [self.targets, self.val_targets]\n",
    "        \n",
    "        NOISY = 0\n",
    "        CLEAN = 1\n",
    "       \n",
    "        #Create the training and validation inputs and targets\n",
    "        for index,data in enumerate(datasetIter):\n",
    "            currNoisyDataset = data[NOISY]\n",
    "            corrCleanDataset = data[CLEAN]\n",
    "            print(f'xFrames (expectedFrames) per audio clip = {len(currNoisyDataset[0])//self.dataconfig.stride_length}')\n",
    "            for idx, currNoisySample in enumerate(tqdm(currNoisyDataset)):\n",
    "                modelInputBuffer = np.zeros((self.dataconfig.modelBufferFrames,fft_freq_bins)).astype(self.complexDtype)\n",
    "                inbuffer = np.zeros((self.dataconfig.frameSize)).astype(self.dtype)\n",
    "                inbufferClean = np.zeros((self.dataconfig.frameSize)).astype(self.dtype)\n",
    "            \n",
    "                for i in range(0, len(currNoisySample),self.dataconfig.stride_length):      \n",
    "                \n",
    "                    if(i+self.dataconfig.stride_length > len(currNoisySample)-1):\n",
    "                        break\n",
    "\n",
    "                    #inbuffer is moved : [__s1__+++++++++++++__s2__] -> [+++++++++++++__s2__]\n",
    "                    inbuffer[:-self.dataconfig.stride_length] = inbuffer[self.dataconfig.stride_length:] \n",
    "                    #inbuffer is filled with new data: [+++++++++++++__s2__] -> [+++++++++++++----]\n",
    "                    inbuffer[-self.dataconfig.stride_length:] = currNoisySample[i : i + self.dataconfig.stride_length]\n",
    "\n",
    "                    inbufferClean[:-self.dataconfig.stride_length] = inbufferClean[self.dataconfig.stride_length:] \n",
    "                    inbufferClean[-self.dataconfig.stride_length:] = corrCleanDataset[idx][i : i + self.dataconfig.stride_length]\n",
    "                    \n",
    "                    #Start up time\n",
    "                    if i < self.dataconfig.frameSize:\n",
    "                        continue\n",
    "\n",
    "                    # ModelInput Creation\n",
    "                    buffer_array = np.array(inbuffer)\n",
    "                    windowed_buffer = buffer_array * hann(len(buffer_array), sym=False)\n",
    "                    frame = np.fft.rfft(windowed_buffer)    \n",
    "                    modelInputBuffer[:-1, :] = modelInputBuffer[1:, :]\n",
    "                    modelInputBuffer[-1, :] = frame\n",
    "                    modelBufferFramesIter[index].append(np.array(modelInputBuffer))\n",
    "\n",
    "                    # Target Creation\n",
    "                    clean_buffer_array = np.array(inbufferClean)\n",
    "                    clean_windowed_buffer = buffer_array * hann(len(clean_buffer_array), sym=False)\n",
    "                    clean_frame = np.fft.rfft(clean_windowed_buffer)\n",
    "                    clean_iffted_segment = np.fft.irfft(clean_frame)\n",
    "                    targetsIter[index].append(clean_iffted_segment)\n",
    "\n",
    "    def createModelBufferInputs3(self):\n",
    "        print(\"CustomDataLoader.createModelBufferInputs3()\")\n",
    "       \n",
    "        fft_freq_bins = int(self.dataconfig.n_fft/2) + 1\n",
    "\n",
    "        datasetIter = [(self.trainNoisyDataset, self.trainCleanDataset), (self.validationNoisyDataset, self.validationCleanDataset)]\n",
    "        modelBufferFramesIter = [self.trainModelInputBuffers, self.val_modelInputBuffers]\n",
    "        phaseInfoIter = [self.trainPhaseInfo, self.val_phaseInfo]\n",
    "        targetsIter = [self.targets, self.val_targets]\n",
    "        \n",
    "        NOISY = 0\n",
    "        CLEAN = 1\n",
    "       \n",
    "        #Create the training and validation inputs and targets\n",
    "        for index,data in enumerate(datasetIter):\n",
    "            currNoisyDataset = data[NOISY]\n",
    "            corrCleanDataset = data[CLEAN]\n",
    "            print(f'xFrames (expectedFrames) per audio clip = {len(currNoisyDataset[0])//self.dataconfig.stride_length}')\n",
    "            for idx, currNoisySample in enumerate(tqdm(currNoisyDataset)):\n",
    "                modelInputBuffer = np.zeros((self.dataconfig.modelBufferFrames,fft_freq_bins)).astype(self.complexDtype)\n",
    "                inbuffer = np.zeros((self.dataconfig.frameSize)).astype(self.dtype)\n",
    "                inbufferClean = np.zeros((self.dataconfig.frameSize)).astype(self.dtype)\n",
    "            \n",
    "                for i in range(0, len(currNoisySample),self.dataconfig.stride_length):      \n",
    "                \n",
    "                    if(i+self.dataconfig.stride_length > len(currNoisySample)-1):\n",
    "                        break\n",
    "\n",
    "                    #inbuffer is moved : [__s1__+++++++++++++__s2__] -> [+++++++++++++__s2__]\n",
    "                    inbuffer[:-self.dataconfig.stride_length] = inbuffer[self.dataconfig.stride_length:] \n",
    "                    #inbuffer is filled with new data: [+++++++++++++__s2__] -> [+++++++++++++----]\n",
    "                    inbuffer[-self.dataconfig.stride_length:] = currNoisySample[i : i + self.dataconfig.stride_length]\n",
    "\n",
    "                    inbufferClean[:-self.dataconfig.stride_length] = inbufferClean[self.dataconfig.stride_length:] \n",
    "                    inbufferClean[-self.dataconfig.stride_length:] = corrCleanDataset[idx][i : i + self.dataconfig.stride_length]\n",
    "                    \n",
    "                    #Start up time\n",
    "                    if i < self.dataconfig.frameSize:\n",
    "                        continue\n",
    "\n",
    "                    # ModelInput Creation\n",
    "                    buffer_array = np.array(inbuffer)\n",
    "                    windowed_buffer = buffer_array * hann(len(buffer_array), sym=False)\n",
    "                    frame = np.fft.rfft(windowed_buffer)    \n",
    "\n",
    "                    # Phase Info\n",
    "                    phaseInfo = np.angle(frame)\n",
    "\n",
    "                    modelInputBuffer[:-1, :] = modelInputBuffer[1:, :]\n",
    "                    modelInputBuffer[-1, :] = frame\n",
    "                    modelBufferFramesIter[index].append(np.array(modelInputBuffer))\n",
    "                    phaseInfoIter[index].append(np.array(phaseInfo))\n",
    "\n",
    "                    # Target Creation\n",
    "                    clean_buffer_array = np.array(inbufferClean)\n",
    "                    clean_windowed_buffer = buffer_array * hann(len(clean_buffer_array), sym=False)\n",
    "                    clean_frame = np.fft.rfft(clean_windowed_buffer)\n",
    "                    targetsIter[index].append(np.abs(clean_frame))\n",
    "\n",
    "    # This function creates the test dataset \n",
    "    def createTestDataset(self):\n",
    "        print(\"CustomDataLoader.createTestDataset()\")\n",
    "        speechSampleSize = self.dataconfig.duration * self.dataconfig.sample_rate\n",
    "        for index,filename in enumerate(self.test_noisy_files_list):\n",
    "            if index == 100:\n",
    "                break\n",
    "\n",
    "            noisySpeech,_ = sf.read(os.path.join(self.dataconfig.noisyPath, filename))\n",
    "            cleanSpeech,_ = sf.read(os.path.join(self.dataconfig.cleanPath, filename))\n",
    "\n",
    "            #Normalize\n",
    "            noisySpeech = minMaxNorm(noisySpeech)\n",
    "            cleanSpeech = minMaxNorm(cleanSpeech)\n",
    "\n",
    "            numSubSamples = int(len(noisySpeech)/speechSampleSize)\n",
    "            for i in range(numSubSamples):\n",
    "                self.testNoisyDataset.append(noisySpeech[i*speechSampleSize:(i+1)*speechSampleSize])\n",
    "                self.testCleanDataset.append(cleanSpeech[i*speechSampleSize:(i+1)*speechSampleSize])\n",
    "\n",
    "        print(\"Test Noisy Dataset Size: \", len(self.testNoisyDataset))\n",
    "        print(\"Test Clean Dataset Size: \", len(self.testCleanDataset))\n",
    "\n",
    "    #Call this function to prepare the dataloader\n",
    "    def prepare(self):\n",
    "        self.createAudioClips()\n",
    "        # self.createModelBufferInputs()\n",
    "        self.createModelBufferInputs2()\n",
    "        self.printMembers()\n",
    "            \n",
    "    def getTrainDataloader(self):\n",
    "        trainingDataset = CustomDataset(\n",
    "            np.array(self.trainModelInputBuffers).astype(self.complexDtype),\n",
    "            np.array(self.targets).astype(self.dtype)\n",
    "            )\n",
    "\n",
    "        return torch.utils.data.DataLoader(trainingDataset,\n",
    "            batch_size = self.dataconfig.batchSize,\n",
    "            shuffle = self.dataconfig.shuffle,\n",
    "            generator = torch.Generator(device= self.dataconfig.device)\n",
    "            )\n",
    "\n",
    "    def getTrainDataloader2(self):\n",
    "        trainingDataset = CustomDataset(\n",
    "            [np.array(self.trainModelInputBuffers).astype(self.complexDtype), np.array(self.trainPhaseInfo).astype(self.dtype)],\n",
    "            np.array(self.targets).astype(self.dtype)\n",
    "            )\n",
    "\n",
    "        return torch.utils.data.DataLoader(trainingDataset,\n",
    "            batch_size = self.dataconfig.batchSize,\n",
    "            shuffle = self.dataconfig.shuffle,\n",
    "            generator = torch.Generator(device= self.dataconfig.device)\n",
    "            )\n",
    "\n",
    "    def getValidationDataloader(self):\n",
    "        validationDataset = CustomDataset(\n",
    "            np.array(self.val_modelInputBuffers).astype(self.complexDtype),\n",
    "            np.array(self.val_targets).astype(self.dtype)\n",
    "            )\n",
    "\n",
    "        return torch.utils.data.DataLoader(validationDataset,\n",
    "                        batch_size = self.dataconfig.batchSize, \n",
    "                        shuffle = self.dataconfig.shuffle,\n",
    "                        generator = torch.Generator(device=self.dataconfig.device)\n",
    "                        )\n",
    "    def getValidationDataloader2(self):\n",
    "        validationDataset = CustomDataset(\n",
    "            [np.array(self.val_modelInputBuffers).astype(self.complexDtype), np.array(self.val_phaseInfo).astype(self.dtype)],\n",
    "            np.array(self.val_targets).astype(self.dtype)\n",
    "            )\n",
    "\n",
    "        return torch.utils.data.DataLoader(validationDataset,\n",
    "                        batch_size = self.dataconfig.batchSize, \n",
    "                        shuffle = self.dataconfig.shuffle,\n",
    "                        generator = torch.Generator(device=self.dataconfig.device)\n",
    "                        )\n",
    "\n",
    "    def printMembers(self):\n",
    "        print('--------------------------DISPLAY---------------------------------------------')\n",
    "        print(f'noisy_files_list.shape = {np.array(self.noisy_files_list).shape}')\n",
    "        print(f'clean_files_list.shape = {np.array(self.clean_files_list).shape}')\n",
    "        print(f'trainNoisyDataset.shape = {len(self.trainNoisyDataset)}')\n",
    "        print(f'trainCleanDataset.shape = {len(self.trainCleanDataset)}')\n",
    "        print(f'trainModelInputBuffers.shape = {len(self.trainModelInputBuffers)},{len(self.trainModelInputBuffers[0])}')\n",
    "        print(f'targets.shape = {len(self.targets)}')\n",
    "\n",
    "        print('-----------------------------------------------------------------------')\n",
    "        print(f'test_noisy_files_list.shape = {np.array(self.test_noisy_files_list).shape}')\n",
    "        print(f'test_clean_files_list.shape = {np.array(self.test_clean_files_list).shape}')\n",
    "        print(f'testNoisyDataset.shape = {len(self.testNoisyDataset)}')\n",
    "        print(f'testCleanDataset.shape = {len(self.testCleanDataset)}')\n",
    "        print('-----------------------------------------------------------------------')\n",
    "        print(f'val_noisy_files_list.shape = {np.array(self.val_noisy_files_list).shape}')\n",
    "        print(f'val_clean_files_list.shape = {np.array(self.val_clean_files_list).shape}')\n",
    "        print(f'validationNoisyDataset.shape = {len(self.validationNoisyDataset)}')\n",
    "        print(f'validationCleanDataset.shape = {len(self.validationCleanDataset)}')\n",
    "        print(f'val_modelInputBuffers.shape = {len(self.val_modelInputBuffers)},{len(self.val_modelInputBuffers[0])}')\n",
    "        print(f'val_targets.shape = {len(self.val_targets)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisyPath = '/home/ubuntu/OticonStuff/dataset/train'\n",
    "cleanPath = '/home/ubuntu/OticonStuff/dataset/y_train'\n",
    "noisy_files_list = fnmatch.filter(os.listdir(noisyPath), '*.wav')\n",
    "clean_files_list = fnmatch.filter(os.listdir(cleanPath), '*.wav')\n",
    "\n",
    "\n",
    "#Split into train and temp ( 70-15-15 split for now)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(noisy_files_list, clean_files_list, test_size=0.3, random_state=42)\n",
    "\n",
    "#Splitting the temp into validation and test\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# print(f'shape of numpy X_train: {np.array(X_train).shape}')\n",
    "# # print(f'shape of numpy y_train: {np.array(y_train).shape}')\n",
    "# print(f'shape of numpy X_test: {np.array(X_test).shape}')\n",
    "# # print(f'shape of numpy y_test: {np.array(y_test).shape}')\n",
    "\n",
    "# print(f'shape of numpy X_val: {np.array(X_val).shape}')\n",
    "# # print(f'shape of numpy y_test: {np.array(y_test).shape}')\n",
    "\n",
    "\n",
    "#All defaults in dataconfig\n",
    "dataConfig = DataConfig(\n",
    "    dtype = torch.float32,\n",
    ")\n",
    "\n",
    "dataloaderCreator = CustomDataloaderCreator(X_train, y_train,X_test,y_test,X_val,y_val,dataconfig=dataConfig)\n",
    "dataloaderCreator.prepare()\n",
    "\n",
    "dataloader = dataloaderCreator.getTrainDataloader()\n",
    "validationDataloader = dataloaderCreator.getValidationDataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'dataloader length = {len(dataloader)}')\n",
    "print(f'validationDataloader length = {len(validationDataloader)}')\n",
    "\n",
    "# Iterate through the dataloader to access individual batches\n",
    "for batch in dataloader:\n",
    "    # Access the shape of the entire batch\n",
    "    modelInputs, targets = batch\n",
    "    randomSelectedTrainingPoint = np.random.randint(0,targets.shape[0])\n",
    "\n",
    "    # Access the shape of the first data point in the batch\n",
    "    print(f'First data point shape = {batch[0].shape}')\n",
    "    print(f'First target shape = {batch[1].shape}')\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
