{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# %%\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "# from torchviz import make_dot\n",
    "import os, fnmatch\n",
    "import torchaudio\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import array\n",
    "import torch.fft as fft\n",
    "from CustomDataloader import CustomDataloaderCreator,DataConfig\n",
    "import tqdm\n",
    "from collections import OrderedDict\n",
    "import wandb\n",
    "from plottingHelper import compareTwoAudios\n",
    "\n",
    "class Trainer():\n",
    "\n",
    "    def __init__(self,model,optimizer,loss_func,num_epochs):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_func = loss_func\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "        self.best_vals = {'psnr': 0.0, 'loss': 1e8}\n",
    "        self.best_model = OrderedDict((k, v.detach().clone()) for k, v in self.model.state_dict().items())\n",
    "\n",
    "    def train(self,train_dataloader,val_dataloader,dataConfig,modelSaveDir,wandbName,debugFlag = False,useWandB = True):\n",
    "        \n",
    "        name = wandbName\n",
    "        #Initializing wandb  \n",
    "\n",
    "        if(useWandB):\n",
    "            wandb.init(\n",
    "                # set the wandb project where this run will be logged\n",
    "                project=\"Shobhit_SEM9\",\n",
    "                name= name,\n",
    "                config={\n",
    "                    \"epochs\": self.num_epochs,\n",
    "                    \"learning_rate\": dataConfig.learningRate,\n",
    "                    \"batch_size\": dataConfig.batchSize,\n",
    "                    \"stride_length\": dataConfig.stride_length,\n",
    "                    \"frame_size\": dataConfig.frameSize,\n",
    "                    \"sample_rate\": dataConfig.sample_rate,\n",
    "                    \"duration\": dataConfig.duration,\n",
    "                    \"n_fft\": dataConfig.n_fft,\n",
    "                    \"modelBufferFrames\": dataConfig.modelBufferFrames,\n",
    "                    \"shuffle\": dataConfig.shuffle,\n",
    "                    \"dtype\": dataConfig.dtype,\n",
    "                },\n",
    "            )\n",
    "\n",
    "        modelPath = f'modelSaveDir/{name}'\n",
    "        fft_freq_bins = int(dataConfig.n_fft/2) + 1\n",
    "        \n",
    "        #Start training loop\n",
    "        with tqdm.trange(self.num_epochs, ncols=100) as t:\n",
    "            for i in t:\n",
    "                # <Inside an epoch>    \n",
    "                #Make sure gradient tracking is on, and do a pass over the data\n",
    "                self.model.train(True)\n",
    "\n",
    "                # Update model\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                running_trainloss = 0.0\n",
    "                #Training loop\n",
    "                randomSelectedBatchNum = np.random.randint(0,len(train_dataloader))\n",
    "                \n",
    "\n",
    "                for batchNum,data in enumerate(train_dataloader):\n",
    "                    # <Inside a batch>  \n",
    "                    modelInputs, targets = data\n",
    "                    randomSelectedTrainingPoint = np.random.randint(0,targets.shape[0])\n",
    "                    # print(f'modelInputs.dtype = {modelInputs.dtype}')\n",
    "\n",
    "                    #ModelInputs here is of type complex64\n",
    "                    if(dataConfig.dtype == torch.float32):\n",
    "                        modelInputs = torch.abs(modelInputs).float()\n",
    "                    else:\n",
    "                        modelInputs = torch.abs(modelInputs).double()\n",
    "                   \n",
    "                    # print(f'modelInputs.dtype = {modelInputs.dtype}')\n",
    "                    #Idk if this is required now\n",
    "                    if(batchNum == len(train_dataloader)):\n",
    "                        break\n",
    "\n",
    "                    reshaped_input = modelInputs.view(modelInputs.shape[0], fft_freq_bins*dataConfig.modelBufferFrames)\n",
    "                    #Model input is (Batchsize, 257*10) :: batch of 10 frames of 257 FFT bins\n",
    "                    ifftedOutputs = self.model(reshaped_input)\n",
    "\n",
    "                    #Model output is (Batchsize, 512) :: batch of single IFFT-ed frame of 257 FFT bins\n",
    "                    if(debugFlag): \n",
    "                        print(f'ifftedOutputs.shape = {ifftedOutputs.shape}')     \n",
    "\n",
    "                    #Taking the first 32 samples from the ifft output\n",
    "                    # firstSamples = ifftedOutputs[:,:dataConfig.stride_length] \n",
    "                    firstSamples = ifftedOutputs\n",
    "\n",
    "                    if(debugFlag):\n",
    "                        print(f'IFFT of model output shape = {ifftedOutputs.shape}')\n",
    "                        print(f'IFFT of model output first {dataConfig.stride_length} samples shape = {firstSamples.shape}')   \n",
    "\n",
    "                    loss = self.loss_func(firstSamples, targets)\n",
    "\n",
    "\n",
    "                    if(batchNum == randomSelectedBatchNum and i%10 ==0):\n",
    "                        compareTwoAudios(firstSamples[randomSelectedTrainingPoint],targets[randomSelectedTrainingPoint],i,randomSelectedBatchNum,logInWandb = useWandB)\n",
    "                        # printQualityScores(targets[5],firstSamples[5],dataConfig.sample_rate)\n",
    "                    \n",
    "                    running_trainloss += loss\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                # <After an epoch> \n",
    "                avg_trainloss = running_trainloss / len(train_dataloader)\n",
    "                # Check for validation loss!\n",
    "                running_vloss = 0.0\n",
    "                # Set the model to evaluation mode\n",
    "                self.model.eval()\n",
    "\n",
    "                # Disable gradient computation and reduce memory consumption.\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    for i,data in enumerate(val_dataloader):\n",
    "                        \n",
    "                        val_modelInputs, val_targets = data\n",
    "\n",
    "                         #val_modelInputs here is of type complex64\n",
    "                        if(dataConfig.dtype == torch.float32):\n",
    "                            val_modelInputs = torch.abs(val_modelInputs).float()\n",
    "                        else:\n",
    "                            val_modelInputs = torch.abs(val_modelInputs).double()\n",
    "                   \n",
    "\n",
    "                        #Idk if this is required now\n",
    "                        if(i == len(val_dataloader)):\n",
    "                            break\n",
    "\n",
    "                        val_reshaped_input = val_modelInputs.view(val_modelInputs.shape[0], fft_freq_bins*dataConfig.modelBufferFrames)\n",
    "                        #Model input is (Batchsize, 257*10) :: batch of 10 frames of 257 FFT bins\n",
    "                        val_ifftedOutputs = self.model(val_reshaped_input)\n",
    "\n",
    "                        #Model output is (Batchsize, 512) :: batch of single IFFT-ed frame of 257 FFT bins\n",
    "                        if(debugFlag): \n",
    "                            print(f'ifftedOutputs.shape = {val_ifftedOutputs.shape}')     \n",
    "\n",
    "                        #Taking the first 32 samples from the ifft output\n",
    "                        # firstSamples = ifftedOutputs[:,:dataConfig.stride_length] \n",
    "                        val_firstSamples = val_ifftedOutputs\n",
    "\n",
    "                        if(debugFlag):\n",
    "                            print(f'IFFT of model output shape = {val_ifftedOutputs.shape}')\n",
    "                            print(f'IFFT of model output first {dataConfig.stride_length} samples shape = {val_firstSamples.shape}')   \n",
    "\n",
    "                        val_loss = self.loss_func(val_firstSamples, val_targets)\n",
    "\n",
    "                        # if(i == len(val_dataloader)/2):\n",
    "                            # compareTwoAudios(val_firstSamples[5],val_targets[5])\n",
    "                        #     # printQualityScores(val_targets[5],val_firstSamples[5],dataConfig.sample_rate)\n",
    "\n",
    "                        running_vloss += val_loss\n",
    "\n",
    "                # Calculate average val loss\n",
    "                avg_vloss = running_vloss / len(val_dataloader)\n",
    "                print('LOSS train {} valid {}'.format(avg_trainloss, avg_vloss))\n",
    "\n",
    "\n",
    "                if(useWandB):\n",
    "                    # Log results to W&B\n",
    "                    wandb.log({\n",
    "                        'trainLoss': avg_trainloss,\n",
    "                        'valLoss': avg_vloss,\n",
    "                    })\n",
    "                \n",
    "                #Save the model if the validation loss is good\n",
    "                if avg_vloss < self.best_vals['loss']:\n",
    "                    self.best_vals['loss'] = avg_vloss\n",
    "                    self.best_model = OrderedDict((k, v.detach().clone()) for k, v in self.model.state_dict().items())\n",
    "                    torch.save(self.best_model, f'{wandbName}.pt')\n",
    "\n",
    "        if(useWandB):\n",
    "            wandb.finish() \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coinpp-new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
