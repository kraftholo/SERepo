{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "# from torchviz import make_dot\n",
    "import os, fnmatch\n",
    "import torchaudio\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import array\n",
    "import torch.fft as fft\n",
    "from CustomDataloader import CustomDataloaderCreator,DataConfig\n",
    "from Training import Trainer\n",
    "from tqdm import tqdm\n",
    "\n",
    "from plottingHelper import compareTwoAudios\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up torch and cuda\n",
    "dtype = torch.float32\n",
    "# dtype = torch.float64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor' if torch.cuda.is_available() else 'torch.FloatTensor')\n",
    "\n",
    "print(device)\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "\n",
    "class DefaultConfig():\n",
    "    def __init__(self):\n",
    "        self.frameSize = 512\n",
    "        self.stride_length = 256\n",
    "        self.learningRate = 0.01\n",
    "        self.batchSize = 64\n",
    "        self.num_epochs = 500\n",
    "        self.shuffle = True\n",
    "        self.duration = 6\n",
    "        self.hiddenSize = 512*4\n",
    "        self.numHiddenLayers = 4\n",
    "        self.filesToUse = 150\n",
    "    \n",
    "    def printMembers(self):\n",
    "        print(\"frameSize: \", self.frameSize)\n",
    "        print(\"stride_length: \", self.stride_length)\n",
    "        print(\"learningRate: \", self.learningRate)\n",
    "        print(\"batchSize: \", self.batchSize)\n",
    "        print(\"num_epochs: \", self.num_epochs)\n",
    "        print(\"shuffle: \", self.shuffle)\n",
    "        print(\"duration: \", self.duration)\n",
    "        print(\"hiddenSize: \", self.hiddenSize)\n",
    "        print(\"numHiddenLayers: \", self.numHiddenLayers)\n",
    "\n",
    "class SimpleLinearModel(nn.Module):\n",
    "    def __init__(self, input_feature_dim,hiddenSize,numHiddenLayers,dtype = torch.float64):\n",
    "        super(SimpleLinearModel, self).__init__()\n",
    "        self.input_feature_dim = input_feature_dim\n",
    "        self.flattenedDims = input_feature_dim[0]*input_feature_dim[1]\n",
    "        layers = []\n",
    "        layers.append(nn.Flatten())\n",
    "\n",
    "        for i in range(numHiddenLayers):\n",
    "            if(i==0):\n",
    "                layers.append(nn.Linear(self.flattenedDims, hiddenSize,dtype=dtype))\n",
    "                layers.append(nn.ReLU())\n",
    "\n",
    "            elif(i==numHiddenLayers-1):\n",
    "                layers.append(nn.Linear(hiddenSize, self.flattenedDims,dtype=dtype))\n",
    "                layers.append(nn.Sigmoid())                                                 # Sigmoid last activation\n",
    "\n",
    "            else:\n",
    "                layers.append(nn.Linear(hiddenSize, hiddenSize,dtype=dtype))\n",
    "                layers.append(nn.ReLU())\n",
    "            \n",
    "\n",
    "        self.neuralnet = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x.view(-1)\n",
    "        # print(\"Input dtype: \", x.dtype)\n",
    "        output = self.neuralnet(x)\n",
    "        op = output.view(-1,*self.input_feature_dim)\n",
    "        # output = fft.irfft(output)\n",
    "        return op\n",
    "    \n",
    "class BasicAutoEncoder(nn.Module):\n",
    "    def __init__(self, input_feature_dim,latentSize,dtype = torch.float64):\n",
    "        super(BasicAutoEncoder, self).__init__()\n",
    "        \n",
    "        self.input_feature_dim = input_feature_dim\n",
    "        self.flattenedDims = input_feature_dim[0]*input_feature_dim[1]\n",
    "        layers = []\n",
    "        layers.append(nn.Flatten())\n",
    "        layers.append(nn.Linear(self.flattenedDims, latentSize*4,dtype=dtype))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(latentSize*4, latentSize*2,dtype=dtype))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(latentSize*2, latentSize,dtype=dtype))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(latentSize, latentSize*2,dtype=dtype))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(latentSize*2, latentSize*4,dtype=dtype))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(latentSize*4, self.flattenedDims,dtype=dtype))\n",
    "        layers.append(nn.Sigmoid())                                                # Sigmoid last activation\n",
    "\n",
    "        self.neuralnet = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(f'Input shape: {x.shape}')            #([BatchSize,257, 376])\n",
    "        op = self.neuralnet(x)\n",
    "        # print(f'Model output shape: {op.shape}')\n",
    "        op = op.view(-1,*self.input_feature_dim)\n",
    "        # print(f'Final output shape: {op.shape}')    # ([BatchSize,257, 376])\n",
    "        return op\n",
    "\n",
    "# https://medium.com/@polanitzer/building-a-cnn-based-autoencoder-with-denoising-in-python-on-gray-scale-images-of-hand-drawn-digits-61131ec492e4\n",
    "class ConvAutoEncoder(nn.Module):\n",
    "    def __init__(self,dtype = torch.float64):\n",
    "        super(ConvAutoEncoder, self).__init__()\n",
    "        # Expected input = (batch_size, inputFFTFrames, numFFTBins)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels= 1, out_channels= 32, kernel_size= (3,3), stride= (1,1), padding= (1,1),dtype=dtype)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size= (2,2), stride= (2,2))\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels= 32, out_channels= 64, kernel_size= (3,3), stride= (1,1), padding= (1,1),dtype=dtype)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size= (2,2), stride= (2,2))\n",
    "\n",
    "        self.t_conv1 = nn.ConvTranspose2d(in_channels= 64, out_channels= 64, kernel_size= (3,3), padding=1,dtype=dtype)\n",
    "        self.upSample1 = nn.Upsample(scale_factor= 2)\n",
    "        self.t_conv2 = nn.ConvTranspose2d(in_channels= 64, out_channels= 32, kernel_size= (3,3), padding=1,dtype=dtype)\n",
    "        self.upSample2 = nn.Upsample(scale_factor= 2)\n",
    "        self.t_conv3 = nn.Conv2d(in_channels= 32, out_channels= 1, kernel_size= (3,3), padding=1,dtype=dtype)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid() \n",
    "\n",
    "    def forward(self,x):\n",
    "        # print(\"Input shape: \", x.shape) # ([BatchSize,257, x])\n",
    "        inputShape = np.copy(x.shape)\n",
    "\n",
    "        reshapedInput = x.unsqueeze(-1) # ([BatchSize, 257, x, 1])\n",
    "        x = reshapedInput.permute(0, 3, 1, 2) # ([BatchSize, 1, 256, x]) One channel image\n",
    "        # print(\"After permute shape: \", x.shape)\n",
    "        x = self.conv1(x)\n",
    "        # print(\"After conv1 shape: \", x.shape)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        # print(\"After maxpool1 shape: \", x.shape)\n",
    "        x = self.conv2(x)\n",
    "        # print(\"After conv2 shape: \", x.shape)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool2(x)\n",
    "        # print(\"After maxpool2 shape: \", x.shape)\n",
    "        x = self.t_conv1(x)\n",
    "        # print(\"After t_conv1 shape: \", x.shape)\n",
    "        x = self.relu(x)\n",
    "        x = self.upSample1(x)\n",
    "        # print(\"After upSample1 shape: \", x.shape)\n",
    "        x = self.t_conv2(x)\n",
    "        # print(\"After t_conv2 shape: \", x.shape)\n",
    "        x = self.relu(x)\n",
    "        x = self.upSample2(x)\n",
    "        # print(\"After upSample2 shape: \", x.shape)\n",
    "        x = self.t_conv3(x)\n",
    "        # print(\"After t_conv3 shape: \", x.shape)\n",
    "        x = self.sigmoid(x)                                                         # Sigmoid last activation   \n",
    "        # print(\"After sigmoid shape: \", x.shape)\n",
    "\n",
    "        masksGenerated = x.permute(0, 2, 3, 1) # ([BatchSize,256, x, 1])\n",
    "        masksGenerated = masksGenerated.squeeze(-1) # ([BatchSize,256, x])\n",
    "        # print(\"masksGenerated shape: \", masksGenerated.shape)\n",
    "\n",
    "        padded_masks = torch.zeros((inputShape[0], inputShape[1], inputShape[2]))\n",
    "        padded_masks[:masksGenerated.shape[0], :masksGenerated.shape[1], :masksGenerated.shape[2]] = masksGenerated\n",
    "\n",
    "        # print(\"Final output shape: \", masksGenerated.shape)\n",
    "        return padded_masks\n",
    "\n",
    "\n",
    "    \n",
    "# model = ConvAutoEncoder(dtype)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisyPath = '/home/ubuntu/OticonStuff/dataset/train'\n",
    "cleanPath = '/home/ubuntu/OticonStuff/dataset/y_train'\n",
    "noisy_files_list = fnmatch.filter(os.listdir(noisyPath), '*.wav')\n",
    "clean_files_list = fnmatch.filter(os.listdir(cleanPath), '*.wav')\n",
    "config = DefaultConfig()\n",
    "\n",
    "# print(\"Number of noisy files: \", len(noisy_files_list))\n",
    "# print(\"Number of clean files: \", len(clean_files_list))\n",
    "# print(\"Noisy file: \", noisy_files_list[1])\n",
    "# print(\"Clean file: \", clean_files_list[1])\n",
    "\n",
    "#Corresponds to 512 -> 32ms\n",
    "frameSize = config.frameSize\n",
    "#Corresponds to 32 -> 2ms\n",
    "stride_length = config.stride_length \n",
    "sampleRate = 16000\n",
    "speechSampleSize = config.duration * sampleRate \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(noisy_files_list, clean_files_list, test_size=0.3, shuffle=True)\n",
    "\n",
    "#Splitting the temp into validation and test\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5,shuffle=True)\n",
    "\n",
    "# print(f'shape of numpy X_train: {np.array(X_train).shape}')\n",
    "# # print(f'shape of numpy y_train: {np.array(y_train).shape}')\n",
    "# print(f'shape of numpy X_test: {np.array(X_test).shape}')\n",
    "# # print(f'shape of numpy y_test: {np.array(y_test).shape}')\n",
    "\n",
    "# print(f'shape of numpy X_val: {np.array(X_val).shape}')\n",
    "# # print(f'shape of numpy y_test: {np.array(y_test).shape}')\n",
    "\n",
    "\n",
    "#All defaults in dataconfig\n",
    "dataConfig = DataConfig(\n",
    "    batchSize= config.batchSize,\n",
    "    dtype= dtype,\n",
    "    device = device,\n",
    "    learningRate = config.learningRate,\n",
    "    frameSize = frameSize, \n",
    "    stride_length = stride_length,\n",
    "    sample_rate = sampleRate,\n",
    "    modelBufferFrames= 0,                   #Doesn't matter for non-RT\n",
    "    duration = config.duration,\n",
    "    n_fft = frameSize,\n",
    "    shuffle = config.shuffle,\n",
    "    noisyPath = noisyPath,\n",
    "    cleanPath = cleanPath\n",
    ")\n",
    "\n",
    "dataloaderCreator = CustomDataloaderCreator(X_train, y_train,X_test,y_test,X_val,y_val,dataconfig=dataConfig,filesToUse=config.filesToUse)\n",
    "dataloaderCreator.prepareNonRT()\n",
    "\n",
    "dataloader = dataloaderCreator.getTrainDataloaderNonRT()\n",
    "validationDataloader = dataloaderCreator.getValidationDataloaderNonRT()\n",
    "\n",
    "numOfFrames = dataloaderCreator.getNumOfFrames()\n",
    "numOfFreqBins = dataloaderCreator.getNumOfFreqBins()\n",
    "dataConfig.numOfFreqBins = numOfFreqBins\n",
    "dataConfig.numOfFrames = numOfFrames\n",
    "\n",
    "selectedTrainExampleFile = X_train[4]\n",
    "print(\"Selected train example file: \", selectedTrainExampleFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of frames: \", numOfFrames)\n",
    "print(\"Number of freq bins: \", numOfFreqBins)\n",
    "\n",
    "# Model config\n",
    "num_epochs = config.num_epochs\n",
    "useScheduler = False\n",
    "useModelNo = 1\n",
    "\n",
    "\n",
    "if(useModelNo == 0):\n",
    "    model = ConvAutoEncoder(dataConfig.dtype)\n",
    "    # runName = f'CNonRT:{dataConfig.frameSize}_stride:{dataConfig.frameSize/2}_LR:{dataConfig.learningRate}_BS:{dataConfig.batchSize}_Epochs:{config.num_epochs}_decayingLR:{useScheduler}_shuffle:{dataConfig.shuffle}_ConvAE'\n",
    "    # runName = f'CNonRT:{dataConfig.frameSize}_stride:{dataConfig.frameSize/2}_LR:{dataConfig.learningRate}_BS:{dataConfig.batchSize}_Epochs:{config.num_epochs}_decayingLR:{useScheduler}_shuffle:{dataConfig.shuffle}_ConvAE_NoAct'\n",
    "    runName = f'CNonRT(TDLOSS):{dataConfig.frameSize}_stride:{dataConfig.frameSize/2}_LR:{dataConfig.learningRate}_BS:{dataConfig.batchSize}_Epochs:{config.num_epochs}_decayingLR:{useScheduler}_shuffle:{dataConfig.shuffle}_ConvAE'\n",
    "elif(useModelNo == 1):\n",
    "    latentsize = 16*8\n",
    "    model = BasicAutoEncoder((numOfFreqBins,numOfFrames),latentSize=latentsize,dtype=dataConfig.dtype) #([BatchSize,257, 376])\n",
    "    # runName = f'CNonRT:{dataConfig.frameSize}_LatentSize:{latentsize}_LR:{dataConfig.learningRate}_BS:{dataConfig.batchSize}_Epochs:{config.num_epochs}_decayingLR:{useScheduler}__shuffle:{dataConfig.shuffle}_BasicAE(moreData)'\n",
    "    # runName = f'CNonRT:{dataConfig.frameSize}_LatentSize:{latentsize}_LR:{dataConfig.learningRate}_BS:{dataConfig.batchSize}_Epochs:{config.num_epochs}_decayingLR:{useScheduler}__shuffle:{dataConfig.shuffle}_BasicAE_NoAct'\n",
    "    runName = f'CNonRT(TDLOSS):{dataConfig.frameSize}_LatentSize:{latentsize}_LR:{dataConfig.learningRate}_BS:{dataConfig.batchSize}_Epochs:{config.num_epochs}_decayingLR:{useScheduler}__shuffle:{dataConfig.shuffle}_BasicAE'\n",
    "else:\n",
    "    model = SimpleLinearModel((numOfFreqBins,numOfFrames),config.hiddenSize,config.numHiddenLayers,dtype=dataConfig.dtype)\n",
    "    # runName = f'CNonRT:{dataConfig.frameSize}_LatentSize:{config.hiddenSize}_LR:{dataConfig.learningRate}_BS:{dataConfig.batchSize}_Epochs:{config.num_epochs}_decayingLR:{useScheduler}_shuffle:{dataConfig.shuffle}_SimpleLinearModel'\n",
    "    # runName = f'CNonRT:{dataConfig.frameSize}_LatentSize:{config.hiddenSize}_LR:{dataConfig.learningRate}_BS:{dataConfig.batchSize}_Epochs:{config.num_epochs}_decayingLR:{useScheduler}_shuffle:{dataConfig.shuffle}_SimpleLinearModel_NoAct'\n",
    "    runName = f'CNonRT(TDLOSS):{dataConfig.frameSize}_LatentSize:{config.hiddenSize}_LR:{dataConfig.learningRate}_BS:{dataConfig.batchSize}_Epochs:{config.num_epochs}_decayingLR:{useScheduler}_shuffle:{dataConfig.shuffle}_SimpleLinearModel'\n",
    "\n",
    "print(\"Model: \", model)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Define your optimizer (e.g., Adam optimizer)\n",
    "optimizer = optim.Adam(model.parameters(), lr=dataConfig.learningRate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.8, verbose=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    optimizer = optimizer,\n",
    "    scheduler = scheduler,\n",
    "    loss_func = loss_fn,\n",
    "    num_epochs = num_epochs,\n",
    "    )\n",
    "\n",
    "\n",
    "# Wandb Controls====================================\n",
    "useWandB = True\n",
    "sweeping = False\n",
    "# ====================================\n",
    "\n",
    "# Loss in the frequency domain \n",
    "# trainer.trainNonRT(\n",
    "#     train_dataloader = dataloader,\n",
    "#     val_dataloader = validationDataloader,\n",
    "#     dataConfig = dataConfig,\n",
    "#     modelSaveDir = \"/home/ubuntu/OticonStuff/models\",\n",
    "#     wandbName = runName,\n",
    "#     wavFileTesting = f'{selectedTrainExampleFile}',\n",
    "#     debugFlag = False,\n",
    "#     useWandB=useWandB,\n",
    "#     sweeping = sweeping,\n",
    "#     useScheduler = useScheduler,\n",
    "#     scaleFactor= 1\n",
    "# )\n",
    "\n",
    "\n",
    "# Loss in the time domain\n",
    "trainer.trainNonRTLossTD(\n",
    "        train_dataloader = dataloader,\n",
    "        val_dataloader = validationDataloader,\n",
    "        dataConfig = dataConfig,\n",
    "        modelSaveDir = \"/home/ubuntu/OticonStuff/models\",\n",
    "        wandbName = runName,\n",
    "        wavFileTesting = f'{selectedTrainExampleFile}',\n",
    "        debugFlag = False,\n",
    "        useWandB=useWandB,\n",
    "        sweeping = sweeping,\n",
    "        useScheduler = useScheduler,\n",
    "        scaleFactor= 1\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coinpp-new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
